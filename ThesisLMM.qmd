---
title: "ThesisLMM"
format: html
editor: visual
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(
  echo = TRUE,      # Don't hide the code
  warning = FALSE,   # Suppress warnings
  message = FALSE    # Suppress messages
)

```

```{r}

library(tidyverse)
library(kableExtra)
library(ggplot2)
library(car)
library(patchwork)
library(dplyr)
library(gridExtra)
library(tidyr)
library(readxl)

allthesis <- read_excel("C:/Users/barne/Downloads/ThesisRData.xlsx", sheet = 1)

allthesis <- allthesis |> 
  mutate(TSS = as.numeric(TSS))

```

```{r}

avgthesis <- allthesis |> 
  group_by(Site, Lake) |>  # Include Lake as a grouping variable
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")

```

# Basic LM Models

First we want to plot the regressions between light attenuation and each of our predictors while also displaying the slope equation, R2, Pvalue, and standard error

```{r}

# Function to get regression statistics (equation, R2, p-value, SE)

get_regression_stats <- function(x, y) {
  model <- lm(y ~ x)
  eq <- paste("y = ", round(coef(model)[2], 2), "x + ", round(coef(model)[1], 2), sep = "")
  rsq <- paste("R² = ", round(summary(model)$r.squared, 3), sep = "")
  pval <- paste("p = ", round(summary(model)$coefficients[2, 4], 3), sep = "")
  se <- paste("SE = ", round(summary(model)$coefficients[2, 2], 3), sep = "")  # Extract SE
  return(list(eq = eq, rsq = rsq, pval = pval, se = se))
}

reg_stats_p1 <- get_regression_stats(avgthesis$TSS, avgthesis$kd)
reg_stats_p2 <- get_regression_stats(avgthesis$CHLa, avgthesis$kd)

```

```{r}

# TSS vs kd

p1 <- ggplot(avgthesis, aes(x = TSS, y = kd)) +
  geom_point(color = "gray70") +  
  geom_smooth(method = "lm", color = "black", se = FALSE) +  
  labs(title = "kd vs TSS", x = "TSS (mg L⁻¹)", y = "kd (m⁻¹)") +
  geom_text(x = max(avgthesis$TSS), y = max(avgthesis$kd), 
            label = paste(reg_stats_p1$eq, "\n", reg_stats_p1$rsq, "\n", reg_stats_p1$pval, "\n", reg_stats_p1$se), 
            hjust = 1, vjust = 2, size = 2, color = "black") + 
  theme_minimal() +
  theme(
    plot.title = element_text(size = 7, color = "black"),  
    axis.title.x = element_text(size = 6, color = "black"), 
    axis.title.y = element_text(size = 6, color = "black"))


# CHLa vs kd

p2 <- ggplot(avgthesis, aes(x = CHLa, y = kd)) +
  geom_point(color = "gray70") +  
  geom_smooth(method = "lm", color = "black", se = FALSE) +  
  labs(title = "kd vs CHLa", x = "CHLa (ug L⁻¹)", y = "kd (m⁻¹)") +
  geom_text(x = max(avgthesis$CHLa), y = max(avgthesis$kd), 
            label = paste(reg_stats_p2$eq, "\n", reg_stats_p2$rsq, "\n", reg_stats_p2$pval, "\n", reg_stats_p2$se), 
            hjust = 1, vjust = 2, size = 2, color = "black") + 
  theme_minimal() +
  theme(
    plot.title = element_text(size = 7, color = "black"),  
    axis.title.x = element_text(size = 6, color = "black"), 
    axis.title.y = element_text(size = 6, color = "black"))

```

To look at the relationship between kd and CDOM, we first need to find the residual kd. We do this by finding the proportion of kd due to TSS, then subtracting that out of our original kd values for each site.

```{r}

# Calculate residual kd

avgthesis <- avgthesis |> 
  mutate(
    Residualkd = kd - (TSS*0.20337)
  )

reg_stats_p3 <- get_regression_stats(avgthesis$CDOM, avgthesis$Residualkd)

```

Now we can perform a regression between CDOM and residual kd then arrange our plots into one output

```{r}

# CDOM vs Residual kd

p3 <- ggplot(avgthesis, aes(x = CDOM, y = Residualkd)) +
  geom_point(color = "gray70") +  
  geom_smooth(method = "lm", color = "black", se = FALSE) +  
  labs(title = "Residual kd vs CDOM", x = "CDOM (m⁻¹)", y = "Residualkd (m⁻¹)") +
  geom_text(x = max(avgthesis$CDOM), y = max(avgthesis$Residualkd), 
            label = paste(reg_stats_p3$eq, "\n", reg_stats_p3$rsq, "\n", reg_stats_p3$pval, "\n", reg_stats_p3$se), 
            hjust = 1, vjust = 2, size = 2, color = "black") + 
  theme_minimal() +
  theme(
    plot.title = element_text(size = 7, color = "black"),  
    axis.title.x = element_text(size = 6, color = "black"), 
    axis.title.y = element_text(size = 6, color = "black"))

# Plot all regressions in the same output

grid.arrange(p1, p2, p3, ncol = 2)

```

To examine these relationships further, we can look at the summaries of each regression

```{r}

p1summary <- lm(kd ~ TSS, data = allthesis)

#summary(p1summary)

p2summary <- lm(kd ~ CHLa, data = avgthesis)

#summary(p2summary)

p3summary <- lm(Residualkd ~ CDOM, data = avgthesis)

#summary(p3summary)

```

We can also look at the diagnostic plots for each of the models, we do this by creating a function and calling that function with each model we want to examine

```{r}
# Function to create diagnostic plots for each model
create_diagnostic_plots <- function(model) {
  par(mfrow = c(2, 2))

  # Residuals vs Fitted plot
  plot(model, which = 1, main = "Residuals vs Fitted")

  # Normal Q-Q plot
  plot(model, which = 2, main = "Normal Q-Q")

  # Scale-Location plot
  plot(model, which = 3, main = "Scale-Location")

  # Residuals vs Leverage plot
  plot(model, which = 5, main = "Residuals vs Leverage")
}


#create_diagnostic_plots(p1summary)

#create_diagnostic_plots(p2summary)

#create_diagnostic_plots(p3summary)

```

Now we want to examine how much light attenuation at each site can be attributed to the effects of TSS and CDOM (our two significant predictors). To do this, we multiply average predictor values for each site by the slope of the predictor relationship, then divide this by the original kd. Finally, we multiply by 100 to convert this proportion to a percentage. 

```{r}

PercentContributions <- avgthesis |>
  mutate(
    `%TSS` = ((TSS*0.20337)/kd)*100,
    `%CDOM` = ((CDOM*0.17382)/kd)*100) |>
  summarize(`Site`, `%TSS`, `%CDOM`)

```

```{r}

# Pivot the data into long format for ggplot
PercentContributions_long <- PercentContributions |> 
  pivot_longer(cols = c(`%TSS`, `%CDOM`), 
               names_to = "Variable", 
               values_to = "PercentContribution")

# Manually recoding the 'Site' variable
PercentContributions_long <- PercentContributions_long |>
  mutate(
    Site = dplyr::recode(Site, 
                         "2-BRI010.78" = "BRI_1",
                         "2-BRI013.12" = "BRI_2",
                         "5ANTW127.14" = "Barfoot",
                         "Chickahominy" = "Chick",
                         "Harrison" = "Harrison",
                         "2-SDY004.27" = "SDY_1",
                         "2-SDY005.85" = "SDY_2",
                         "2-MBN000.96" = "SDY_3",
                         "2-SFT031.08" = "SFT_1",
                         "2-DYC000.19" = "SFT_2",
                         "2-SFT033.42" = "SFT_3",
                         "2-SFT034.38" = "SFT_4",
                         "Dam Surface" = "ANNA"))

# Change the site order for plot visualization
site_order <- c( "Harrison", "Chick", "BRI_1", "BRI_2", "SDY_1", "SDY_2", "SDY_3", "SFT_1", "SFT_2", "SFT_3", "SFT_4", "Barfoot", "ANNA")

PercentContributions_long <- PercentContributions_long |>
  mutate(Site = factor(Site, levels = site_order))



```

```{r}

ggplot(PercentContributions_long, aes(x = Site, y = PercentContribution, fill = Variable)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    title = "% kd Contributions of TSS and CDOM by Site",
    x = "Site",
    y = "% kd",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  


```

# Multivariate Linear Regression

```{r}

response <- "kd"
predictors <- c("TSS", "CDOM", "CHLa")  

# Generate all combinations of predictors
all_combinations <- unlist(
  lapply(0:length(predictors), function(x) combn(predictors, x, simplify = FALSE)), 
  recursive = FALSE
)

model_results <- lapply(all_combinations, function(predictor_set) {

  formula <- as.formula(paste(response, "~", ifelse(length(predictor_set) == 0, "1", paste(predictor_set, collapse = " + "))))

  model <- lm(formula, data = avgthesis)
  
  list(
    predictors = predictor_set,
    model = model,
    AIC = AIC(model))})

# Convert results to df
results_df <- do.call(rbind, lapply(model_results, function(x) {
  data.frame(
    Predictors = paste(x$predictors, collapse = " + "),
    AIC = x$AIC)}))

# Find the model with the lowest AIC
min_AIC <- min(results_df$AIC)

# Calculate the delta AIC for each model
results_df <- results_df |>
  mutate(Delta_AIC = AIC - min_AIC) |>
  arrange(Delta_AIC)

print(results_df)

```

```{r}

lm_tsscdom <- lm(kd ~ TSS + CDOM, data = avgthesis)
summary(lm_tsscdom)

```

```{r}

# Effects of CHLa are still insignificant here, slopes are comparable to CDOM+TSS model

lm_allpred <- lm(kd ~ TSS + CDOM + CHLa, data = avgthesis)
summary(lm_allpred)

```


# Linear Mixed Effect Models

Here we want to use other variables as random effects to account for the nested design of the study: we have Dates within Sites within Lakes

```{r}

library(lme4)
library(MuMIn) 
library(lmerTest)

```

```{r}

lmm1tss <- lmer(kd ~ CDOM + CHLa + TSS + (1|Date), data = allthesis)
summary(lmm1tss)

```

```{r}

# Get the fixed effect estimates
fixed_effects <- fixef(lmm1tss)

# Get the summary
summary_lmm1tss <- summary(lmm1tss)

# Extract p-values (if available)
p_values <- if ("Pr(>|t|)" %in% colnames(summary_lmm1tss$coefficients)) {
  summary_lmm1tss$coefficients[, "Pr(>|t|)"]
} else {
  NA
}

# Extract standard errors
se_values <- summary_lmm1tss$coefficients[, "Std. Error"]

# R-squared
r_squared_values <- r.squaredGLMM(lmm1tss)

# Print full equation
equation <- paste0("y = ", round(fixed_effects["(Intercept)"], 4))
for (name in names(fixed_effects)[-1]) {
  equation <- paste0(equation, " + ", round(fixed_effects[name], 4), "*", name)
}

# Display
cat("\nThe relationship equation is:\n", equation, "\n\n")

for (name in names(fixed_effects)[-1]) {
  cat(paste0("Predictor: ", name, "\n"))
  cat("  Coefficient: ", round(fixed_effects[name], 4), "\n")
  cat("  P-value: ", round(p_values[name], 4), "\n")
  cat("  Std. Error: ", round(se_values[name], 4), "\n\n")
}

cat("Marginal R-squared: ", round(r_squared_values[1], 4), "\n")
cat("Conditional R-squared: ", round(r_squared_values[2], 4), "\n")

```

Now we can also look at the residual plots for this model

```{r}

#plot(lmm1tss)
#qqnorm(resid(lmm1tss))
#qqline(resid(lmm1tss))

```

# Changing Date Format

```{r}

# Change format of Date to be Month/Year instead of Year/Month/Day
# This will allow us to account for "Date" as a random effect
# This can't currently be done as reservoirs were sampled on different days in each month

library(lubridate)

allthesis <- allthesis %>%
  mutate(Date = as.Date(Date),          
         Date = format(Date, "%m/%Y"))  


```

Using just Date as a random effect

```{r}

lmmall <- lmer(kd ~ CDOM + CHLa + TSS + (1|Date), data = allthesis)
summary(lmmall)

#anova(lmm1tss)

```

Using both Date and Lake as random effects

```{r}

lmmall2 <- lmer(kd ~ CDOM + CHLa + TSS + (1|Date/Lake), data = allthesis)
summary(lmmall2)

```

Using average data instead of all data (Lake random effect)

```{r}

lmmavg1 <- lmer(kd ~ CDOM + CHLa + TSS + (1|Lake), data = avgthesis)
summary(lmmavg1)

```

```{r}

AIC(lmmall, lmmall2, lmmavg1)

```



# Scaling Data then LMM

It is good practice to standardise your explanatory variables before proceeding so that they have a mean of zero (“centering”) and standard deviation of one (“scaling”). It ensures that the estimated coefficients are all on the same scale, making it easier to compare effect sizes. 

scale() centers the data (the column mean is subtracted from the values in the column) and then scales it (the centered column values are divided by the column’s standard deviation).


```{r}

#hist(allthesis$kd)
#hist(allthesis$TSS)
#hist(allthesis$CHLa)
#hist(allthesis$CDOM)

```

Here we scale our data, indicated by "predictor"1:

```{r}

allthesis$kd1 <- scale(allthesis$kd, center = TRUE, scale = TRUE)
#hist(allthesis$kd1)

allthesis$TSS1 <- scale(allthesis$TSS, center = TRUE, scale = TRUE)
#hist(allthesis$TSS1)

allthesis$CHLa1 <- scale(allthesis$CHLa, center = TRUE, scale = TRUE)
#hist(allthesis$CHLa1)

allthesis$CDOM1 <- scale(allthesis$CDOM, center = TRUE, scale = TRUE)
#hist(allthesis$CDOM1)

```

Here we try to normalize/standardize our predictors further by log transforming data first, then scaling, indicated by "predictor"2

```{r}

allthesis$kd2 <- scale(allthesis$kd, center = TRUE, scale = TRUE)
#hist(allthesis$kd2)

allthesis$TSS2 <- scale(log(allthesis$TSS), center = TRUE, scale = TRUE)
#hist(allthesis$TSS2)

allthesis$CHLa2 <- scale(log(allthesis$CHLa), center = TRUE, scale = TRUE)
#hist(allthesis$CHLa2)

allthesis$CDOM2 <- scale(log(allthesis$CDOM), center = TRUE, scale = TRUE)
#hist(allthesis$CDOM2)

```

```{r}

# Scaled Predictors, Date grouping

lmmall3 <- lmer(kd1 ~ CDOM1 + CHLa1 + TSS1 + (1|Date), data = allthesis)
summary(lmmall3)

```

```{r}

# Scaled Predictors, Date and Lake grouping

lmmall4 <- lmer(kd1 ~ CDOM1 + CHLa1 + TSS1 + (1|Date/Lake), data = allthesis)
summary(lmmall4)

```

```{r}

#Log transformed and scaled predictors, Date grouping

lmmall5 <- lmer(kd2 ~ CDOM2 + CHLa2 + TSS2 + (1|Date), data = allthesis)
summary(lmmall5)

```

```{r}

#Log transformed and scaled predictors, Date and Lake grouping

lmmall6 <- lmer(kd2 ~ CDOM2 + CHLa2 + TSS2 + (1|Date/Lake), data = allthesis)
summary(lmmall6)

```

```{r}

anova(lmmall, lmmall2, lmmall3, lmmall4, lmmall5, lmmall6)

```

So according to this anova, the best model would be lmmall2 or lmmall (both using data that hasn't been scaled) based on AIC values

# Scaling average data

```{r}

avgthesis$kd1 <- scale(avgthesis$kd, center = TRUE, scale = TRUE)
#hist(allthesis$kd1)

avgthesis$TSS1 <- scale(avgthesis$TSS, center = TRUE, scale = TRUE)
#hist(allthesis$TSS1)

avgthesis$CHLa1 <- scale(avgthesis$CHLa, center = TRUE, scale = TRUE)
#hist(allthesis$CHLa1)

avgthesis$CDOM1 <- scale(avgthesis$CDOM, center = TRUE, scale = TRUE)
#hist(allthesis$CDOM1)

```

```{r}

avgthesis$kd2 <- scale(avgthesis$kd, center = TRUE, scale = TRUE)
#hist(allthesis$kd2)

avgthesis$TSS2 <- scale(log(avgthesis$TSS), center = TRUE, scale = TRUE)
#hist(allthesis$TSS2)

avgthesis$CHLa2 <- scale(log(avgthesis$CHLa), center = TRUE, scale = TRUE)
#hist(allthesis$CHLa2)

avgthesis$CDOM2 <- scale(log(avgthesis$CDOM), center = TRUE, scale = TRUE)
#hist(allthesis$CDOM2)

```

```{r}

lmmavg2 <- lmer(kd1 ~ CDOM1 + CHLa1 + TSS1 + (1|Lake), data = avgthesis)
summary(lmmavg2)

```

```{r}

lmmavg3 <- lmer(kd2 ~ CDOM2 + CHLa2 + TSS2 + (1|Lake), data = avgthesis)
summary(lmmavg3)

```

```{r}

anova(lmmavg1, lmmavg2, lmmavg3)

```

```{r}

AIC(lm_tsscdom, lmmavg1)

```

Best model for avg data is "lm_tsscdom"

It appears as though for both average data and all data models, the best models are those that do not have scaled data or log data. lmmall2 uses both lake and date as random effects, while lm_tsscdom is just a simple multivariate linear regression (no random effects)

```{r}

AIC(lm_tsscdom, lmmall2)

```

Between those two models, it appears that the simple multivariate regression is the best.

# Comparison of multivariate predictor contributions vs univariate residual method

```{r}

#summary(lm_tsscdom)

#TSS slope = 0.17402
#CDOM slope = 0.17916

PercentContributions <- avgthesis |>
  mutate(
    `%TSS` = ((TSS*0.20337)/kd)*100,
    `%CDOM` = ((CDOM*0.17382)/kd)*100,
    `mult%TSS` = ((TSS*0.17402)/kd)*100,
    `mult%CDOM` = ((CDOM*0.17916)/kd)*100) |>
  summarize(`Site`, `%TSS`, `%CDOM`, `mult%TSS`, `mult%CDOM`)

head(PercentContributions)

```

```{r}

# Paired t-test for %TSS vs mult%TSS
t_test_tss <- t.test(PercentContributions$`%TSS`, PercentContributions$`mult%TSS`, paired = TRUE)

# Paired t-test for %CDOM vs mult%CDOM
t_test_cdom <- t.test(PercentContributions$`%CDOM`, PercentContributions$`mult%CDOM`, paired = TRUE)

t_test_tss
t_test_cdom

```

According to the results of the paired Ttests there are statistical differences in our percent kd explained by predictors based on univariate vs multivariate results. 

Doing univariate regressions and subtracting out the residuals gives us estimates of TSS effects that is ~8% higher than multivariate estimates on average.

Alternatively, univariate estimates of CDOM effects are ~0.84% lower than multivariate estimates on average.

Comparing percent contributions between multivariate and univariate (hierarchical) regressions is tricky as to do so, we have to be sure that our predictors are not correlated. Multivariate regressions are order independent and partition variance simultaneously.

# Does order matter?

I know that changing the order in which I do my univariate regressions changes the percent estimates for each predictor. Theoretically, I don't think this should be the case since TSS and CDOM should not be correlated, one represents scattering effects and one represents absorption effects. 

Here I want to run a quick test to determine if there actually is a correlation between TSS and CDOM that may be causing the order of our regressions to change our findings. 

```{r}

cor(avgthesis[, c("TSS", "CDOM")])

```

Here, it appears as though CDOM and TSS are minorly correlated indicating that there may be some shared variance between predictors. While this isn't a lot, it is likely enough to make the sequential univariate regressions order sensitive. Due to these being slightly correlated, it may be better to use a multivariate approach. 

```{r}

library(car)
vif(lm(kd ~ TSS + CDOM, data = avgthesis))


```
A VIF of 1 indicates no correlation between predictors, this value (1.03) means there is negligible correlation and that the slopes of this multivariate regression can be treated as independent contributions to kd. 


